{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Gym version v0.24.1 has a number of critical issues with `gym.make` such that environment observation and action spaces are incorrectly evaluated, raising incorrect errors and warning . It is recommend to downgrading to v0.23.1 or upgrading to v0.25.1\n",
      "/Users/ismaelrobles-razzaq/Desktop/research/.venv/lib/python3.11/site-packages/gym/envs/registration.py:396: UserWarning: \u001b[33mWARN: The `registry.all` method is deprecated. Please use `registry.values` instead.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import gym\n",
    "import neurogym as ngym\n",
    "from neurogym.wrappers import ScheduleEnvs\n",
    "from neurogym.utils import scheduler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from neurogym.wrappers.block import MultiEnvs\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import matplotlib\n",
    "from matplotlib.colors import ListedColormap\n",
    "import copy\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the 1 task and multitask arrays by loading model, running function (files too big to transfer easily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(Net, self).__init__()\n",
    "        self.rnn = nn.RNN(input_size, hidden_size)\n",
    "        self.linear = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, hidden = self.rnn(x)\n",
    "        x = self.linear(out)\n",
    "        return x, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrix(model, dataset, num_trials=10000, timesteps=10, hidden_size=None):\n",
    "\n",
    "    inputs, labels = dataset()\n",
    "\n",
    "    input_size = inputs.shape[2]\n",
    "    label_size = labels.shape[1]\n",
    "    output_size = 1\n",
    "      \n",
    "    timesteps = labels.shape[0]\n",
    "    print(f\"inputs_size: {input_size}, label_size: {label_size}, output_size: {output_size}, hidden_size: {hidden_size}, timesteps = {timesteps}\")\n",
    "\n",
    "    matrix_size = (num_trials, timesteps, input_size + label_size + output_size + hidden_size)\n",
    "    output_matrix = np.zeros(matrix_size)\n",
    "    model.eval()\n",
    "\n",
    "    for trial in range(num_trials):\n",
    "\n",
    "        inputs, labels = dataset()  \n",
    "\n",
    "        inputs = torch.tensor(inputs, dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            hidden_states, _ = model.rnn(inputs)\n",
    "            output, _ = model(inputs)\n",
    "            _, predicted = torch.max(output.data, -1) \n",
    "\n",
    "\n",
    "            for t in range(timesteps):\n",
    "                output_matrix[trial, t, :input_size] = inputs[t, 0, :].numpy()\n",
    "                output_matrix[trial, t, input_size:input_size+label_size] = labels[t].numpy()\n",
    "                output_matrix[trial, t, input_size+label_size:input_size+label_size+output_size] = predicted[t, 0].numpy()\n",
    "                output_matrix[trial, t, -hidden_size:] = hidden_states[0, 0, :].numpy()\n",
    "\n",
    "    return output_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset \n",
    "kwargs = {}\n",
    "kwargs['timing'] = {\n",
    "                'fixation': 300,\n",
    "                'stimulus': 500,\n",
    "                'decision': 200} # fix timing for discrete trial intervals\n",
    "\n",
    "task_env = ngym.make('yang19.dm1-v0', **kwargs)\n",
    "dataset = ngym.Dataset(task_env, batch_size=1, seq_len=10)\n",
    "env = dataset.env\n",
    "\n",
    "ob_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/4b964bn507zc652npm2l2ch80000gn/T/ipykernel_22475/390546136.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  net.load_state_dict(torch.load('small_net.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (rnn): RNN(33, 20)\n",
       "  (linear): Linear(in_features=20, out_features=17, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# small model\n",
    "net = Net(input_size=env.observation_space.shape[0],\n",
    "          hidden_size=20,\n",
    "          output_size=act_size)\n",
    "\n",
    "net.load_state_dict(torch.load('small_net.pth'))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_size: 33, label_size: 1, output_size: 1, hidden_size: 20, timesteps = 10\n"
     ]
    }
   ],
   "source": [
    "small_array = generate_matrix(net, dataset, hidden_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multitask array\n",
    "# change batch size to 1 for simplicity\n",
    "task_names = ['yang19.go-v0', 'yang19.rtgo-v0', 'yang19.dlygo-v0', 'yang19.anti-v0', 'yang19.rtanti-v0', 'yang19.dlyanti-v0', 'yang19.dm1-v0', 'yang19.dm2-v0', 'yang19.ctxdm1-v0', 'yang19.ctxdm2-v0', 'yang19.multidm-v0', 'yang19.dlydm1-v0', 'yang19.dlydm2-v0', 'yang19.ctxdlydm1-v0', 'yang19.ctxdlydm2-v0', 'yang19.multidlydm-v0', 'yang19.dms-v0', 'yang19.dnms-v0', 'yang19.dmc-v0', 'yang19.dnmc-v0']\n",
    "task_list = []\n",
    "for name in task_names:\n",
    "    task_list.append(ngym.make(name))\n",
    "\n",
    "\n",
    "# Create a schedule for switching between tasks\n",
    "schedule = scheduler.RandomSchedule(n=len(task_list))\n",
    "\n",
    "# Combine the tasks\n",
    "combined_env = ScheduleEnvs(task_list, schedule=schedule, env_input=True)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = ngym.Dataset(combined_env, batch_size=1, seq_len=100)\n",
    "env = dataset.env\n",
    "\n",
    "ob_size = env.observation_space.shape[0]\n",
    "act_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/4b964bn507zc652npm2l2ch80000gn/T/ipykernel_22475/2886405219.py:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  multitask_net.load_state_dict(torch.load('multitask_net.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multitask_net = Net(input_size=env.observation_space.shape[0],\n",
    "          hidden_size=256,\n",
    "          output_size=act_size)\n",
    "multitask_net.load_state_dict(torch.load('multitask_net.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs_size: 53, label_size: 1, output_size: 1, hidden_size: 256, timesteps = 100\n"
     ]
    }
   ],
   "source": [
    "multitask_array = generate_matrix(multitask_net, dataset, hidden_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Navigating Array\n",
    "Array dims: (trial, timestep, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 100, 311)\n",
      "(10000, 10, 55)\n"
     ]
    }
   ],
   "source": [
    "print(multitask_array.shape)\n",
    "print(small_array.shape)\n",
    "\n",
    "# Optional: save files, uncomment to run\n",
    "#np.save('multitask_array', multitask_array)\n",
    "#np.save('small_array', small_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 55)\n",
      "(55,)\n"
     ]
    }
   ],
   "source": [
    "# Access 1 trial:\n",
    "print(small_array[0].shape) \n",
    "# Access 1 timestep of the trial:\n",
    "print(small_array[0, 0].shape) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unpacking trial data:\n",
    "data = inputs, label, predicted, hidden states\n",
    "so for the small 1 task rnn with input size 33, label size 1, prediction size 1, and 20 units/hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = small_array[0, 0]\n",
    "inputs = data[:33]\n",
    "label = data[33:34]\n",
    "prediction = data[34:35]\n",
    "hidden_states = data[35:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large multitask rnn with input size 53 (33 normal inputs + 20 one hot encoded task vectors), label size 1, prediction size 1, and 256 units/hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = multitask_array[0, 0]\n",
    "inputs = data[:53] # fixation input then 2 size 16 rings, then 20 one hot encoded task vector\n",
    "label = data[53:54]\n",
    "prediction = data[54:55]\n",
    "hidden_states = data[55:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
